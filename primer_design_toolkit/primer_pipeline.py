#!/usr/bin/env python3
"""
Complete Bacterial Genome Analysis Pipeline: Download -> Prokka -> Roary -> Primer Design
Supports resuming from any step in the workflow
"""

import os
import sys
import subprocess
import glob
import shutil
import argparse
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

class GenomePipeline:
    """
    Complete pipeline for bacterial genome analysis and primer design
    
    Workflow:
    1. Download genomes from NCBI
    2. Annotate with Prokka 
    3. Pangenome analysis with Roary
    4. Identify species-specific genes
    5. Extract gene sequences
    6. Multiple sequence alignment with MAFFT
    7. Primer design with Primer3
    8. Parse Primer3 results
    9. Quality scoring and ranking
    """
    def __init__(self, genus, outgroup_genera=None, level="Complete Genome", threads=4, fast_mode=True, assembly_summary_path=None, max_genes=None):
        """
        Initialize the pipeline
        
        Args:
            genus (str): Target genus name
            outgroup_genera (list): List of outgroup genus names
            level (str): Assembly level filter
            threads (int): Number of parallel threads
            fast_mode (bool): Use fast mode for Roary analysis
            assembly_summary_path (str): Path to assembly_summary.txt file
            max_genes (int): Maximum number of genes to process (None for all genes)
        """
        self.genus = genus
        self.genus_safe = genus.replace(" ", "_")
        self.outgroup_genera = outgroup_genera or []
        self.outgroup_genera_safe = [g.replace(" ", "_") for g in (outgroup_genera or [])]
        self.level = level
        self.threads = threads
        self.fast_mode = fast_mode  # Control roary speed mode
        self.assembly_summary_path = assembly_summary_path
        self.max_genes = max_genes  # Maximum number of genes to process
        self.work_dir = Path.cwd()
        self.genus_dir = self.work_dir / self.genus_safe
        self.data_dir = self.genus_dir / "data"
        self.target_dir = self.data_dir / self.genus_safe[:3].capitalize()
        self.outgroup_dir = self.data_dir / "outgroup"
        self.prokka_dir = self.genus_dir / "prokka_results"
        self.roary_dir = self.genus_dir / "roary_results"
        
    def log(self, message, level="INFO"):
        """
        Log messages with timestamp
        
        Args:
            message (str): Log message
            level (str): Log level (INFO, ERROR, etc.)
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        self.genus_dir.mkdir(parents=True, exist_ok=True)
        
        # Write to log file
        log_file = self.genus_dir / "pipeline.log"
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] {level}: {message}\n")
    
    def check_conda_env(self, env_name):
        """æ£€æŸ¥condaç¯å¢ƒæ˜¯å¦å­˜åœ¨"""
        try:
            result = subprocess.run(
                ["conda", "env", "list"], 
                capture_output=True, 
                text=True, 
                check=True
            )
            return env_name in result.stdout
        except subprocess.CalledProcessError:
            return False
    
    def activate_conda_env(self, env_name):
        """æ¿€æ´»condaç¯å¢ƒ / Activate conda environment"""
        if not self.check_conda_env(env_name):
            self.log(f"âŒ Conda environment '{env_name}' not found", "ERROR")
            return False
        
        self.log(f"ğŸ”„ Activating conda environment: {env_name}")
        return True
    
    def run_command_in_env(self, command, env_name, cwd=None):
        """åœ¨æŒ‡å®šcondaç¯å¢ƒä¸­è¿è¡Œå‘½ä»¤ / Run command in specified conda environment"""
        if not self.activate_conda_env(env_name):
            return False
            
        # æ„å»ºåœ¨condaç¯å¢ƒä¸­è¿è¡Œçš„å‘½ä»¤ / Build command to run in conda environment
        if isinstance(command, list):
            cmd_str = " ".join(command)
        else:
            cmd_str = command
            
        full_command = f"conda run -n {env_name} {cmd_str}"
        
        try:
            self.log(f"ğŸ”„ Executing command: {cmd_str}")
            result = subprocess.run(
                full_command,
                shell=True,
                cwd=cwd or self.work_dir,
                check=True,
                capture_output=False
            )
            return True
        except subprocess.CalledProcessError as e:
            self.log(f"âŒ Command execution failed: {e}", "ERROR")
            return False
    
    def step1_download(self):
        """æ­¥éª¤1: ä¸‹è½½åŸºå› ç»„ / Step 1: Download genomes"""
        self.log("ğŸš€ Starting Step 1: Genome download")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½å®Œæˆ / Check if download is already completed
        if self.target_dir.exists() and list(self.target_dir.glob("*.fna")):
            self.log("âœ… Target genomes already exist, skipping download")
            target_downloaded = True
        else:
            target_downloaded = False
            
        if self.outgroup_genera and self.outgroup_dir.exists() and list(self.outgroup_dir.glob("*.fna")):
            self.log("âœ… Outgroup genomes already exist, skipping download")
            outgroup_downloaded = True
        else:
            outgroup_downloaded = False
            
        if target_downloaded and (not self.outgroup_genera or outgroup_downloaded):
            self.log("âœ… All genomes already downloaded")
            # æ£€æŸ¥å¹¶æ˜¾ç¤ºå·²æœ‰çš„summaryæ–‡ä»¶ / Check and display existing summary files
            self._check_existing_summary_files()
            return True
        
        # ä½¿ç”¨æ–°çš„GenomeDownloader / Use new GenomeDownloader
        try:
            from .genome_downloader import GenomeDownloader
            
            downloader = GenomeDownloader(
                work_dir=str(self.work_dir),
                assembly_summary_path=self.assembly_summary_path
            )
            
            # ä¸‹è½½ç›®æ ‡åŸºå› ç»„å’Œå¤–ç¾¤ / Download target genomes and outgroups
            success = downloader.download_with_outgroup(
                target_genus=self.genus,
                outgroup_genera=self.outgroup_genera,
                level=self.level,
                threads=self.threads
            )
            
            if success:
                self.log("âœ… Step 1 completed: Genome download")
                # æ˜¾ç¤ºsummaryæ–‡ä»¶ä½ç½® / Display summary file locations
                self._display_summary_files()
                return True
            else:
                self.log("âŒ Genome download failed", "ERROR")
                return False
                
        except Exception as e:
            self.log(f"âŒ Error during download: {e}", "ERROR")
            return False
    
    def _check_existing_summary_files(self):
        """æ£€æŸ¥å¹¶æ˜¾ç¤ºå·²å­˜åœ¨çš„summaryæ–‡ä»¶ / Check and display existing summary files"""
        summary_files = []
        
        # Check target genus summary file
        target_summary = self.genus_dir / f"{self.genus_safe}_genome_summary.csv"
        if target_summary.exists():
            summary_files.append(f"Target genome summary: {target_summary}")
        
        # Check outgroup summary files
        for outgroup, outgroup_safe in zip(self.outgroup_genera, self.outgroup_genera_safe):
            outgroup_dir = self.work_dir / outgroup_safe
            outgroup_summary = outgroup_dir / f"{outgroup_safe}_genome_summary.csv"
            if outgroup_summary.exists():
                summary_files.append(f"Outgroup genome summary: {outgroup_summary}")
        
        if summary_files:
            self.log("ğŸ“Š Genome summary files saved:")
            for summary_file in summary_files:
                self.log(f"   {summary_file}")
        else:
            self.log("âš ï¸  No genome summary file found")
    
    def _display_summary_files(self):
        """æ˜¾ç¤ºæ–°ç”Ÿæˆçš„summaryæ–‡ä»¶ä½ç½® / Display newly generated summary file locations"""
        summary_files = []
        
        # æ£€æŸ¥ç›®æ ‡genusçš„summaryæ–‡ä»¶ / Check target genus summary file
        target_summary = self.genus_dir / f"{self.genus_safe}_genome_summary.csv"
        if target_summary.exists():
            summary_files.append(f"Target genome summary: {target_summary}")
        
        # æ£€æŸ¥å¤–ç¾¤çš„summaryæ–‡ä»¶ / Check outgroup summary files
        for outgroup, outgroup_safe in zip(self.outgroup_genera, self.outgroup_genera_safe):
            outgroup_dir = self.work_dir / outgroup_safe
            outgroup_summary = outgroup_dir / f"{outgroup_safe}_genome_summary.csv"
            if outgroup_summary.exists():
                summary_files.append(f"Outgroup genome summary: {outgroup_summary}")
        
        if summary_files:
            self.log("ğŸ“Š Genome summary information saved:")
            for summary_file in summary_files:
                self.log(f"   {summary_file}")
        else:
            self.log("âš ï¸  No genome summary file found")
    
    def step2_prokka(self):
        """æ­¥éª¤2: Prokkaæ³¨é‡Š (å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬) / Step 2: Prokka annotation (parallel optimized version)"""
        self.log("ğŸš€ Starting Step 2: Prokka annotation")
        
        # åˆ›å»ºprokkaç»“æœç›®å½• / Create prokka results directory
        self.prokka_dir.mkdir(exist_ok=True)
        
        # Collect all fna files that need annotation
        fna_files = []
        
        # ç›®æ ‡åŸºå› ç»„æ–‡ä»¶ / Target genome files
        if self.target_dir.exists():
            fna_files.extend(list(self.target_dir.glob("*.fna")))
        
        # å¤–ç¾¤æ–‡ä»¶ / Outgroup files
        if self.outgroup_dir.exists():
            fna_files.extend(list(self.outgroup_dir.glob("*.fna")))
        
        if not fna_files:
            self.log("âŒ No fna files found for annotation", "ERROR")
            return False
        
        self.log(f"ğŸ“‹ Found {len(fna_files)} genomes for annotation")
        
        # æ£€æŸ¥å·²å®Œæˆçš„æ³¨é‡Š / Check completed annotations
        remaining_files = []
        for fna_file in fna_files:
            sample_name = fna_file.stem
            gff_file = self.prokka_dir / sample_name / f"{sample_name}.gff"
            if gff_file.exists():
                self.log(f"âœ… {sample_name} annotation already exists, skipping")
            else:
                remaining_files.append(fna_file)
        
        if not remaining_files:
            self.log("âœ… All genome annotations completed")
            return True
        
        self.log(f"ğŸ”„ Need to annotate {len(remaining_files)} genomes")
        
        # ä¸€æ¬¡æ€§æ£€æŸ¥prokkaç¯å¢ƒ / Check prokka environment once
        if not self.check_conda_env("prokka"):
            self.log("âŒ Conda environment 'prokka' not found", "ERROR")
            return False
        
        # ä¼˜åŒ–å¹¶è¡Œç­–ç•¥å’ŒCPUåˆ†é… / Optimized parallel strategy and CPU allocation
        if len(remaining_files) <= 3:
            # å°‘é‡åŸºå› ç»„ï¼šä¸²è¡Œå¤„ç†ï¼Œæ¯ä¸ªä½¿ç”¨æ›´å¤šCPU / Few genomes: serial processing with more CPUs each
            max_workers = 1
            cpus_per_prokka = min(self.threads, 16)  # å•ä¸ªProkkaè¿›ç¨‹æœ€å¤šä½¿ç”¨16ä¸ªCPU / Single Prokka process uses max 16 CPUs
            self.log(f"ğŸš€ Using serial processing strategy: {cpus_per_prokka} CPUs per genome")
        else:
            # å¤šä¸ªåŸºå› ç»„ï¼šå¹¶è¡Œå¤„ç†ï¼Œä½†é™åˆ¶å¹¶å‘æ•°ä»¥é¿å…èµ„æºç«äº‰
            # Multiple genomes: parallel processing with limited concurrency to avoid resource contention
            max_workers = min(max(1, self.threads // 8), len(remaining_files))  # æ¯ä¸ªè¿›ç¨‹è‡³å°‘8ä¸ªCPU / At least 8 CPUs per process
            cpus_per_prokka = max(1, self.threads // max_workers)
            cpus_per_prokka = min(cpus_per_prokka, 16)  # é™åˆ¶æœ€å¤§CPUæ•° / Limit max CPU count
            self.log(f"ğŸš€ Using parallel processing strategy: {max_workers} parallel processes, {cpus_per_prokka} CPUs each")
        
        # å¹¶è¡Œæ‰§è¡Œprokkaæ³¨é‡Š / Execute prokka annotation in parallel
        def run_single_prokka(fna_file):
            """è¿è¡Œå•ä¸ªåŸºå› ç»„çš„prokkaæ³¨é‡Š / Run prokka annotation for single genome"""
            sample_name = fna_file.stem
            output_dir = self.prokka_dir / sample_name
            
            cmd = [
                "conda", "run", "-n", "prokka",
                "prokka",
                "--outdir", str(output_dir),
                "--prefix", sample_name,
                "--kingdom", "Bacteria",
                "--cpus", str(cpus_per_prokka),  # ä½¿ç”¨é¢„è®¡ç®—çš„CPUæ•°é‡ / Use pre-calculated CPU count
                "--force",
                str(fna_file)
            ]
            
            try:
                self.log(f"ğŸ”„ Starting annotation: {sample_name} (using {cpus_per_prokka} CPUs)")
                result = subprocess.run(
                    cmd,
                    cwd=self.work_dir,
                    check=True,
                    capture_output=True,
                    text=True
                )
                self.log(f"âœ… Annotation completed: {sample_name}")
                return sample_name, True, None
            except subprocess.CalledProcessError as e:
                error_msg = f"stderr: {e.stderr[:200]}..." if e.stderr else str(e)
                self.log(f"âŒ Annotation failed {sample_name}: {error_msg}", "ERROR")
                return sample_name, False, error_msg
        
        # è®¡ç®—åˆé€‚çš„å¹¶è¡Œæ•°é‡ / Calculate appropriate parallel count
        # æ€»CPUæ•°é™¤ä»¥æ¯ä¸ªprokkaè¿›ç¨‹çš„CPUæ•°ï¼Œä½†ä¸è¶…è¿‡åŸºå› ç»„æ•°é‡ / Total CPUs divided by CPUs per prokka process, but not exceeding genome count
        max_workers = min(self.threads, len(remaining_files))
        self.log(f"ğŸš€ Using {max_workers} parallel processes for annotation")
        
        # æ‰§è¡Œå¹¶è¡Œæ³¨é‡Š / Execute parallel annotation
        success_count = 0
        failed_samples = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡ / Submit all tasks
            future_to_file = {
                executor.submit(run_single_prokka, fna_file): fna_file 
                for fna_file in remaining_files
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡ / Process completed tasks
            for future in as_completed(future_to_file):
                fna_file = future_to_file[future]
                try:
                    sample_name, success, error = future.result()
                    if success:
                        success_count += 1
                    else:
                        failed_samples.append(sample_name)
                except Exception as exc:
                    sample_name = fna_file.stem
                    self.log(f"âŒ {sample_name} execution error: {exc}", "ERROR")
                    failed_samples.append(sample_name)
        
        # æŠ¥å‘Šç»“æœ / Report results
        self.log(f"ğŸ“Š Annotation completion statistics: {success_count}/{len(remaining_files)} successful")
        if failed_samples:
            self.log(f"âŒ Failed samples: {', '.join(failed_samples)}", "ERROR")
            return False
        
        self.log("âœ… Step 2 completed: Prokka annotation")
        return True
    
    def step3_roary(self):
        """æ­¥éª¤3: Roaryæ³›åŸºå› ç»„åˆ†æ (ä¼˜åŒ–ç‰ˆæœ¬)"""
        self.log("ğŸš€ å¼€å§‹æ­¥éª¤3: Roaryæ³›åŸºå› ç»„åˆ†æ")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆ - æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ç»“æœç›®å½•
        result_files_to_check = [
            self.roary_dir / "gene_presence_absence.csv",
            self.work_dir / "roary_out" / "gene_presence_absence.csv"
        ]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰çš„roaryç»“æœç›®å½•ï¼ˆåŒ…æ‹¬å¸¦æ—¶é—´æˆ³çš„ï¼‰
        existing_roary_dirs = list(self.work_dir.glob("roary_*"))
        for roary_dir in existing_roary_dirs:
            if (roary_dir / "gene_presence_absence.csv").exists():
                self.log(f"âœ… å‘ç°å·²å®Œæˆçš„Roaryåˆ†æ: {roary_dir}")
                return True
        
        for result_file in result_files_to_check:
            if result_file.exists():
                self.log(f"âœ… Roaryåˆ†æå·²å®Œæˆ: {result_file.parent}")
                return True
        
        # æ”¶é›†æ‰€æœ‰gffæ–‡ä»¶
        gff_files = list(self.prokka_dir.glob("*/*.gff"))
        
        if not gff_files:
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°gffæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡ŒProkkaæ³¨é‡Š", "ERROR")
            return False
        
        self.log(f"ğŸ“‹ æ‰¾åˆ° {len(gff_files)} ä¸ªgffæ–‡ä»¶ç”¨äºæ³›åŸºå› ç»„åˆ†æ")
        
        # ä¸€æ¬¡æ€§æ£€æŸ¥roaryç¯å¢ƒ
        if not self.check_conda_env("roary"):
            self.log("âŒ Condaç¯å¢ƒ 'roary' ä¸å­˜åœ¨", "ERROR")
            return False
        
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ç©ºç›®å½•ï¼Œé¿å…roaryåˆ›å»ºæ—¶é—´æˆ³ç›®å½•
        if self.roary_dir.exists():
            try:
                # å¦‚æœç›®å½•ä¸ºç©ºæˆ–ä¸åŒ…å«ä¸»è¦ç»“æœæ–‡ä»¶ï¼Œåˆ é™¤å®ƒ
                if not any(self.roary_dir.iterdir()) or not (self.roary_dir / "gene_presence_absence.csv").exists():
                    shutil.rmtree(self.roary_dir)
                    self.log(f"ğŸ§¹ æ¸…ç†ç©ºçš„roaryç›®å½•: {self.roary_dir}")
            except Exception as e:
                self.log(f"âš ï¸  æ¸…ç†ç›®å½•æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        # ä¸è¦é¢„å…ˆåˆ›å»ºè¾“å‡ºç›®å½•ï¼Œè®©Roaryè‡ªå·±åˆ›å»º
        # ç§»é™¤è¿™è¡Œ: self.roary_dir.mkdir(exist_ok=True)
        
        # è¿è¡ŒRoary - æ ¹æ®æ¨¡å¼é€‰æ‹©å‚æ•°
        if self.fast_mode:
            # å¿«é€Ÿæ¨¡å¼ï¼šé€‚åˆåˆæ­¥åˆ†æå’Œå¤§è§„æ¨¡æ•°æ®
            mode_desc = "å¿«é€Ÿæ¨¡å¼ (-n)"
            mode_params = [
                "-e",           # åˆ›å»ºå¤šåºåˆ—æ¯”å¯¹
                "-n",           # å¿«é€Ÿæ ¸å¿ƒåŸºå› æ¯”å¯¹
            ]
        else:
            # é«˜è´¨é‡æ¨¡å¼ï¼šé€‚åˆæœ€ç»ˆåˆ†æå’Œå¼•ç‰©è®¾è®¡
            mode_desc = "é«˜è´¨é‡æ¨¡å¼ (--mafft)"
            mode_params = [
                "-e",           # åˆ›å»ºå¤šåºåˆ—æ¯”å¯¹
                "--mafft",      # ä½¿ç”¨MAFFTè¿›è¡Œé«˜è´¨é‡æ¯”å¯¹
            ]
        
        cmd = [
            "conda", "run", "-n", "roary",
            "roary",
        ] + mode_params + [
            "-p", str(self.threads),  # çº¿ç¨‹æ•°
            "-i", "95",     # èº«ä»½é˜ˆå€¼95% (å¼•ç‰©è®¾è®¡éœ€è¦æ›´é«˜ä¿å®ˆæ€§)
            "-cd", "100",   # æ ¸å¿ƒåŸºå› å®šä¹‰é˜ˆå€¼100% (å¼•ç‰©ç›®æ ‡å¿…é¡»åœ¨æ‰€æœ‰åŸºå› ç»„ä¸­å­˜åœ¨)
            "-v",           # è¯¦ç»†è¾“å‡º
            "-f", str(self.roary_dir)  # è¾“å‡ºç›®å½• - Roaryä¼šåˆ›å»ºè¿™ä¸ªç›®å½•
        ] + [str(f) for f in gff_files]
        
        try:
            self.log(f"ğŸ”„ è¿è¡ŒRoaryæ³›åŸºå› ç»„åˆ†æ - {mode_desc}")
            self.log(f"ğŸ“ Roaryå°†åˆ›å»ºè¾“å‡ºç›®å½•: {self.roary_dir}")
            result = subprocess.run(
                cmd,
                cwd=self.work_dir,
                check=True,
                capture_output=False  # æ˜¾ç¤ºå®æ—¶è¾“å‡º
            )
            
            # éªŒè¯ç»“æœæ–‡ä»¶æ˜¯å¦æˆåŠŸåˆ›å»º
            if (self.roary_dir / "gene_presence_absence.csv").exists():
                self.log("âœ… æ­¥éª¤3å®Œæˆ: Roaryæ³›åŸºå› ç»„åˆ†æ")
                return True
            else:
                self.log("âŒ Roaryè¿è¡Œå®Œæˆä½†æœªæ‰¾åˆ°é¢„æœŸçš„ç»“æœæ–‡ä»¶", "ERROR")
                return False
                
        except subprocess.CalledProcessError as e:
            self.log(f"âŒ Roaryåˆ†æå¤±è´¥: {e}", "ERROR")
            return False
    
    def step4_find_specific_genes(self):
        """æ­¥éª¤4: æ‰¾å‡ºåªåœ¨ç›®æ ‡genusä¸­å­˜åœ¨è€Œåœ¨å¤–ç¾¤ä¸­ä¸å­˜åœ¨çš„åŸºå›  (å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬)"""
        self.log("ğŸš€ å¼€å§‹æ­¥éª¤4: ç­›é€‰ç›®æ ‡ç‰¹å¼‚æ€§åŸºå› ")
        
        # æ£€æŸ¥roaryç»“æœæ˜¯å¦å­˜åœ¨
        gene_presence_file = None
        roary_dirs = [self.roary_dir]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–roaryç»“æœç›®å½•
        existing_roary_dirs = list(self.work_dir.glob("roary_*"))
        roary_dirs.extend(existing_roary_dirs)
        
        for roary_dir in roary_dirs:
            potential_file = roary_dir / "gene_presence_absence.csv"
            if potential_file.exists():
                gene_presence_file = potential_file
                break
        
        if not gene_presence_file:
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°gene_presence_absence.csvæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡ŒRoaryåˆ†æ", "ERROR")
            return False
        
        self.log(f"ğŸ“‹ ä½¿ç”¨åŸºå› å­˜åœ¨/ç¼ºå¤±çŸ©é˜µ: {gene_presence_file}")
        
        # åˆ†æåŸºå› å­˜åœ¨/ç¼ºå¤±æ¨¡å¼
        import pandas as pd
        df = pd.read_csv(gene_presence_file, low_memory=False)
        
        # è·å–æ‰€æœ‰åˆ—åï¼ˆæ ·æœ¬åï¼‰
        sample_columns = [col for col in df.columns if col not in 
                         ['Gene', 'Non-unique Gene name', 'Annotation', 'No. isolates', 'No. sequences', 
                          'Avg sequences per isolate', 'Genome Fragment', 'Order within Fragment', 
                          'Accessory Fragment', 'Accessory Order with Fragment', 'QC', 'Min group size nuc', 
                          'Max group size nuc', 'Avg group size nuc']]
        
        self.log(f"ğŸ“Š å‘ç° {len(sample_columns)} ä¸ªæ ·æœ¬")
        
        # ç®€å•è€Œå¯é çš„æ ·æœ¬åˆ†ç±»ï¼šç›®æ ‡ vs å¤–ç¾¤
        target_samples = []
        outgroup_samples = []
        
        # ç›®æ ‡genusçš„å‰ç¼€
        target_prefix = self.genus[:3].capitalize() + "_"
        
        self.log(f"ğŸ¯ ç›®æ ‡å‰ç¼€: {target_prefix}")
        
        # åˆ†ç±»æ ·æœ¬ï¼šæ˜¯ç›®æ ‡å‰ç¼€çš„ä¸ºç›®æ ‡ï¼Œå…¶ä»–éƒ½æ˜¯å¤–ç¾¤
        for sample in sample_columns:
            if sample.startswith(target_prefix):
                target_samples.append(sample)
            else:
                outgroup_samples.append(sample)
        
        # æ—¥å¿—è¾“å‡ºè¯†åˆ«ç»“æœ
        self.log(f"ğŸ¯ ç›®æ ‡åŸºå› ç»„: {len(target_samples)} ä¸ª")
        if target_samples:
            self.log(f"   æ ·æœ¬: {', '.join(target_samples[:3])}{'...' if len(target_samples) > 3 else ''}")
        
        self.log(f"ğŸ”— å¤–ç¾¤åŸºå› ç»„: {len(outgroup_samples)} ä¸ª")
        if outgroup_samples:
            self.log(f"   æ ·æœ¬: {', '.join(outgroup_samples[:3])}{'...' if len(outgroup_samples) > 3 else ''}")
        
        if not target_samples:
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°ç›®æ ‡åŸºå› ç»„æ ·æœ¬", "ERROR")
            return False
        
        # æœ€ç»ˆç»Ÿè®¡
        self.log(f"ğŸ“Š æœ€ç»ˆåˆ†ç±»ç»“æœ:")
        self.log(f"   ğŸ¯ ç›®æ ‡åŸºå› ç»„: {len(target_samples)} ä¸ª")
        self.log(f"   ğŸ”— å¤–ç¾¤åŸºå› ç»„: {len(outgroup_samples)} ä¸ª")
        
        # å¹¶è¡Œç­›é€‰ç‰¹å¼‚æ€§åŸºå› 
        def check_gene_specificity(gene_data):
            """å¹¶è¡Œæ£€æŸ¥å•ä¸ªåŸºå› çš„ç‰¹å¼‚æ€§"""
            try:
                idx, row = gene_data
                gene_name = row['Gene']
                annotation = str(row.get('Annotation', '')).lower()
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ‰€æœ‰ç›®æ ‡åŸºå› ç»„ä¸­å­˜åœ¨
                target_present = all(pd.notna(row[sample]) and str(row[sample]).strip() != '' 
                                   for sample in target_samples)
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ‰€æœ‰å¤–ç¾¤ä¸­ä¸å­˜åœ¨
                if outgroup_samples:
                    outgroup_absent = all(pd.isna(row[sample]) or str(row[sample]).strip() == '' 
                                        for sample in outgroup_samples)
                else:
                    outgroup_absent = True  # å¦‚æœæ²¡æœ‰å¤–ç¾¤ï¼Œåˆ™è®¤ä¸ºç¬¦åˆæ¡ä»¶
                
                is_specific = target_present and outgroup_absent
                is_hypothetical = 'hypothetical protein' in annotation
                
                return idx, {
                    'gene_name': gene_name,
                    'annotation': annotation,
                    'is_specific': is_specific,
                    'is_hypothetical': is_hypothetical,
                    'target_present': target_present,
                    'outgroup_absent': outgroup_absent
                }, True, None
                
            except Exception as e:
                return idx, {}, False, str(e)
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰åŸºå› 
        self.log("ğŸ”„ å¼€å§‹å¹¶è¡Œç­›é€‰åŸºå› ...")
        max_workers = min(self.threads, len(df))
        self.log(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹è¿›è¡ŒåŸºå› ç­›é€‰")
        
        gene_results = {}
        failed_count = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰åŸºå› æ£€æŸ¥ä»»åŠ¡
            future_to_gene = {
                executor.submit(check_gene_specificity, (idx, row)): idx 
                for idx, row in df.iterrows()
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_gene):
                gene_idx = future_to_gene[future]
                try:
                    idx, gene_info, success, error = future.result()
                    if success:
                        gene_results[idx] = gene_info
                    else:
                        failed_count += 1
                        if error:
                            self.log(f"âŒ åŸºå›  {idx} ç­›é€‰å¤±è´¥: {error}", "ERROR")
                except Exception as exc:
                    failed_count += 1
                    self.log(f"âŒ åŸºå›  {gene_idx} ç­›é€‰å‡ºé”™: {exc}", "ERROR")
        
        # æ”¶é›†ç­›é€‰ç»“æœ
        all_specific_genes = []
        hypothetical_specific_genes = []
        
        for idx, gene_info in gene_results.items():
            if gene_info.get('is_specific', False):
                gene_name = gene_info['gene_name']
                all_specific_genes.append(gene_name)
                
                if gene_info.get('is_hypothetical', False):
                    hypothetical_specific_genes.append(gene_name)
        
        self.log(f"âœ… å‘ç° {len(all_specific_genes)} ä¸ªç›®æ ‡ç‰¹å¼‚æ€§åŸºå› ")
        self.log(f"ğŸ¯ å…¶ä¸­ {len(hypothetical_specific_genes)} ä¸ªä¸ºhypothetical protein")
        if failed_count > 0:
            self.log(f"âŒ å¤±è´¥åŸºå› æ•°: {failed_count}")
        
        # ä¼˜å…ˆä½¿ç”¨hypothetical proteinï¼Œå¦‚æœæ•°é‡å¤ªå°‘åˆ™ä½¿ç”¨æ‰€æœ‰ç‰¹å¼‚æ€§åŸºå› 
        min_genes_threshold = 5  # è‡³å°‘éœ€è¦5ä¸ªåŸºå› æ‰è®¤ä¸ºè¶³å¤Ÿ
        
        if len(hypothetical_specific_genes) >= min_genes_threshold:
            selected_genes = hypothetical_specific_genes
            self.log(f"ğŸ” ä¼˜å…ˆä½¿ç”¨ {len(selected_genes)} ä¸ªhypothetical proteinåŸºå› è¿›è¡Œå¼•ç‰©è®¾è®¡")
        else:
            selected_genes = all_specific_genes
            self.log(f"âš ï¸  hypothetical proteinåŸºå› æ•°é‡ä¸è¶³({len(hypothetical_specific_genes)} < {min_genes_threshold})")
            self.log(f"ğŸ”„ ä½¿ç”¨æ‰€æœ‰ {len(selected_genes)} ä¸ªç‰¹å¼‚æ€§åŸºå› è¿›è¡Œå¼•ç‰©è®¾è®¡")
        
        # ä¿å­˜é€‰å®šçš„ç‰¹å¼‚æ€§åŸºå› åˆ—è¡¨
        specific_genes_file = self.genus_dir / "specific_genes.txt"
        with open(specific_genes_file, 'w') as f:
            for gene in selected_genes:
                f.write(f"{gene}\n")
        
        # åŒæ—¶ä¿å­˜è¯¦ç»†çš„åŸºå› ä¿¡æ¯
        detailed_genes_file = self.genus_dir / "specific_genes_detailed.csv"
        selected_df = df[df['Gene'].isin(selected_genes)][['Gene', 'Annotation']].copy()
        selected_df.to_csv(detailed_genes_file, index=False)
        
        self.log(f"ğŸ“ é€‰å®šåŸºå› åˆ—è¡¨ä¿å­˜åˆ°: {specific_genes_file}")
        self.log(f"ğŸ“ è¯¦ç»†åŸºå› ä¿¡æ¯ä¿å­˜åˆ°: {detailed_genes_file}")
        
        if len(selected_genes) == 0:
            self.log("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç›®æ ‡ç‰¹å¼‚æ€§åŸºå› ï¼Œæ— æ³•è¿›è¡Œå¼•ç‰©è®¾è®¡", "ERROR")
            return False
        
        return True
    
    def step5_extract_gene_sequences(self):
        """æ­¥éª¤5: æå–åŸºå› åºåˆ— (å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬)"""
        self.log("ğŸš€ å¼€å§‹æ­¥éª¤5: æå–åŸºå› åºåˆ—")
        
        # æ£€æŸ¥ç‰¹å¼‚æ€§åŸºå› åˆ—è¡¨
        specific_genes_file = self.genus_dir / "specific_genes.txt"
        if not specific_genes_file.exists():
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°specific_genes.txtæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤4", "ERROR")
            return False
        
        # è¯»å–ç‰¹å¼‚æ€§åŸºå› åˆ—è¡¨
        with open(specific_genes_file, 'r') as f:
            specific_genes = [line.strip() for line in f if line.strip()]
        
        # ç¡®å®šå®é™…è¦å¤„ç†çš„åŸºå› æ•°é‡
        genes_to_process = specific_genes[:self.max_genes] if self.max_genes else specific_genes
        total_genes = len(specific_genes)
        processing_genes = len(genes_to_process)
        
        if self.max_genes and total_genes > self.max_genes:
            self.log(f"ğŸ“‹ æ€»å…±æœ‰ {total_genes} ä¸ªç‰¹å¼‚æ€§åŸºå› ï¼Œé™åˆ¶å¤„ç†å‰ {self.max_genes} ä¸ª")
        else:
            self.log(f"ğŸ“‹ å¤„ç†æ‰€æœ‰ {processing_genes} ä¸ªç‰¹å¼‚æ€§åŸºå› ")
        
        # åˆ›å»ºåºåˆ—æå–ç›®å½•
        extract_dir = self.genus_dir / "gene_sequences"
        extract_dir.mkdir(exist_ok=True)
        
        # ä½¿ç”¨ç°æœ‰çš„extractè„šæœ¬é€»è¾‘
        import pandas as pd
        from Bio import SeqIO
        
        # æŸ¥æ‰¾gene_presence_absence.csv
        gene_presence_file = None
        for roary_dir in [self.roary_dir] + list(self.work_dir.glob("roary_*")):
            potential_file = roary_dir / "gene_presence_absence.csv"
            if potential_file.exists():
                gene_presence_file = potential_file
                break
        
        if not gene_presence_file:
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°gene_presence_absence.csvæ–‡ä»¶", "ERROR")
            return False
        
        df = pd.read_csv(gene_presence_file, low_memory=False)
        
        # è·å–æ ·æœ¬åˆ—å
        sample_columns = [col for col in df.columns if col not in 
                         ['Gene', 'Non-unique Gene name', 'Annotation', 'No. isolates', 'No. sequences', 
                          'Avg sequences per isolate', 'Genome Fragment', 'Order within Fragment', 
                          'Accessory Fragment', 'Accessory Order with Fragment', 'QC', 'Min group size nuc', 
                          'Max group size nuc', 'Avg group size nuc']]
        
        # æŸ¥æ‰¾ffnæ–‡ä»¶
        ffn_files = {}
        prokka_results = list(self.prokka_dir.glob("*/*.ffn"))
        
        for ffn_file in prokka_results:
            sample_name = ffn_file.parent.name
            # å°è¯•åŒ¹é…æ ·æœ¬å
            for col in sample_columns:
                if col in sample_name or sample_name in col:
                    ffn_files[col] = ffn_file
                    break
        
        self.log(f"ğŸ“ æ‰¾åˆ° {len(ffn_files)} ä¸ªffnæ–‡ä»¶")
        
        if not ffn_files:
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ffnæ–‡ä»¶", "ERROR")
            return False
        
        # é¢„è¯»æ‰€æœ‰åºåˆ—
        self.log("ğŸ”„ é¢„åŠ è½½åºåˆ—æ–‡ä»¶...")
        seqdicts = {}
        
        def load_sequences(sample_ffn_pair):
            """å¹¶è¡ŒåŠ è½½åºåˆ—æ–‡ä»¶"""
            sample, ffn_file = sample_ffn_pair
            try:
                seqdict = SeqIO.to_dict(SeqIO.parse(ffn_file, "fasta"))
                self.log(f"âœ… åŠ è½½åºåˆ—æ–‡ä»¶: {sample}")
                return sample, seqdict, True, None
            except Exception as e:
                self.log(f"âŒ åŠ è½½åºåˆ—æ–‡ä»¶å¤±è´¥ {sample}: {e}", "ERROR")
                return sample, None, False, str(e)
        
        # å¹¶è¡ŒåŠ è½½åºåˆ—æ–‡ä»¶
        max_workers = min(self.threads, len(ffn_files))
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_sample = {
                executor.submit(load_sequences, (sample, ffn_file)): sample 
                for sample, ffn_file in ffn_files.items()
            }
            
            for future in as_completed(future_to_sample):
                sample = future_to_sample[future]
                try:
                    sample_name, seqdict, success, error = future.result()
                    if success:
                        seqdicts[sample_name] = seqdict
                except Exception as exc:
                    self.log(f"âŒ {sample} åºåˆ—åŠ è½½å‡ºé”™: {exc}", "ERROR")
        
        # å¹¶è¡Œæå–ç‰¹å¼‚æ€§åŸºå› çš„åºåˆ—
        def extract_gene_sequences(gene):
            """å¹¶è¡Œå¤„ç†å•ä¸ªåŸºå› çš„åºåˆ—æå–"""
            try:
                gene_row = df[df['Gene'] == gene].iloc[0]
                seqs = []
                
                for sample in sample_columns:
                    if sample in seqdicts and pd.notna(gene_row[sample]):
                        gene_id = str(gene_row[sample]).strip()
                        if gene_id in seqdicts[sample]:
                            seq_record = seqdicts[sample][gene_id]
                            seq_record.id = f"{sample}_{gene_id}"
                            seq_record.description = f"{sample} {gene}"
                            seqs.append(seq_record)
                
                if len(seqs) >= 2:  # è‡³å°‘éœ€è¦2ä¸ªåºåˆ—æ‰èƒ½è¿›è¡Œæ¯”å¯¹ / At least 2 sequences needed for alignment
                    output_file = extract_dir / f"{gene}.fa"
                    SeqIO.write(seqs, output_file, "fasta")
                    self.log(f"âœ… Extracted gene sequences: {gene} ({len(seqs)} sequences)")
                    return gene, len(seqs), True, None
                else:
                    self.log(f"âš ï¸  Insufficient gene sequences: {gene} (only {len(seqs)} sequences)")
                    return gene, len(seqs), False, "Insufficient sequence count"
            
            except Exception as e:
                self.log(f"âŒ æå–åŸºå› åºåˆ—å¤±è´¥ {gene}: {e}", "ERROR")
                return gene, 0, False, str(e)
        
        # å¹¶è¡Œæå–åŸºå› åºåˆ—
        self.log("ğŸ”„ å¼€å§‹å¹¶è¡Œæå–åŸºå› åºåˆ—...")
        extracted_count = 0
        failed_genes = []
        
        max_workers = min(self.threads, processing_genes)
        self.log(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹è¿›è¡Œåºåˆ—æå–")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰åºåˆ—æå–ä»»åŠ¡
            future_to_gene = {
                executor.submit(extract_gene_sequences, gene): gene 
                for gene in genes_to_process
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_gene):
                gene = future_to_gene[future]
                try:
                    gene_name, seq_count, success, error = future.result()
                    if success:
                        extracted_count += 1
                    else:
                        failed_genes.append(gene_name)
                except Exception as exc:
                    self.log(f"âŒ {gene} åºåˆ—æå–å‡ºé”™: {exc}", "ERROR")
                    failed_genes.append(gene)
        
        self.log(f"âœ… æ­¥éª¤5å®Œæˆ: æˆåŠŸæå– {extracted_count}/{processing_genes} ä¸ªåŸºå› çš„åºåˆ—")
        if failed_genes:
            self.log(f"âŒ å¤±è´¥çš„åŸºå› : {', '.join(failed_genes[:5])}{'...' if len(failed_genes) > 5 else ''}")
        
        return extracted_count > 0
    
    def step6_find_conserved_regions(self):
        """æ­¥éª¤6: å¤šåºåˆ—æ¯”å¯¹å¹¶æ‰¾åˆ°ä¿å®ˆåŒºåŸŸ (å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬)"""
        self.log("ğŸš€ å¼€å§‹æ­¥éª¤6: å¤šåºåˆ—æ¯”å¯¹å’Œä¿å®ˆåŒºåŸŸè¯†åˆ«")
        
        extract_dir = self.genus_dir / "gene_sequences"
        if not extract_dir.exists():
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°åŸºå› åºåˆ—ç›®å½•ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤5", "ERROR")
            return False
        
        # åˆ›å»ºæ¯”å¯¹ç»“æœç›®å½•
        alignment_dir = self.genus_dir / "alignments"
        alignment_dir.mkdir(exist_ok=True)
        
        # è·å–æ‰€æœ‰åŸºå› åºåˆ—æ–‡ä»¶
        gene_files = list(extract_dir.glob("*.fa"))
        if not gene_files:
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°åŸºå› åºåˆ—æ–‡ä»¶", "ERROR")
            return False
        
        self.log(f"ğŸ“‹ éœ€è¦æ¯”å¯¹ {len(gene_files)} ä¸ªåŸºå› ")
        
        # æ£€æŸ¥mafftç¯å¢ƒ
        try:
            result = subprocess.run(["mafft", "--version"], capture_output=True, text=True)
            self.log("âœ… MAFFTå¯ç”¨")
        except FileNotFoundError:
            self.log("âŒ æœªæ‰¾åˆ°MAFFTç¨‹åºï¼Œè¯·å®‰è£…MAFFT", "ERROR")
            return False
        
        # å¹¶è¡Œæ‰§è¡Œå¤šåºåˆ—æ¯”å¯¹
        def align_single_gene(gene_file):
            """å¹¶è¡Œå¤„ç†å•ä¸ªåŸºå› çš„æ¯”å¯¹"""
            gene_name = gene_file.stem
            alignment_file = alignment_dir / f"{gene_name}_aln.fa"
            
            try:
                self.log(f"ğŸ”„ æ­£åœ¨æ¯”å¯¹: {gene_name}")
                
                # è¿è¡ŒMAFFTæ¯”å¯¹
                with open(gene_file, 'r') as input_f, open(alignment_file, 'w') as output_f:
                    result = subprocess.run(
                        ["mafft", "--auto", "--quiet", str(gene_file)],
                        stdout=output_f,
                        stderr=subprocess.PIPE,
                        text=True,
                        check=True
                    )
                
                self.log(f"âœ… å®Œæˆæ¯”å¯¹: {gene_name}")
                return gene_name, alignment_file, True, None
                
            except Exception as e:
                self.log(f"âŒ æ¯”å¯¹å¤±è´¥ {gene_name}: {e}", "ERROR")
                return gene_name, None, False, str(e)
        
        # è®¡ç®—åˆé€‚çš„å¹¶è¡Œæ•°é‡
        max_workers = min(self.threads, len(gene_files))
        self.log(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹è¿›è¡Œæ¯”å¯¹")
        
        # æ‰§è¡Œå¹¶è¡Œæ¯”å¯¹
        aligned_count = 0
        alignment_files = []
        failed_genes = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ¯”å¯¹ä»»åŠ¡
            future_to_gene = {
                executor.submit(align_single_gene, gene_file): gene_file 
                for gene_file in gene_files
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_gene):
                gene_file = future_to_gene[future]
                try:
                    gene_name, alignment_file, success, error = future.result()
                    if success:
                        alignment_files.append(alignment_file)
                        aligned_count += 1
                    else:
                        failed_genes.append(gene_name)
                except Exception as exc:
                    gene_name = gene_file.stem
                    self.log(f"âŒ {gene_name} æ‰§è¡Œå‡ºé”™: {exc}", "ERROR")
                    failed_genes.append(gene_name)
        
        if aligned_count == 0:
            self.log("âŒ æ²¡æœ‰æˆåŠŸçš„æ¯”å¯¹ç»“æœ", "ERROR")
            return False
        
        self.log(f"ğŸ“Š æˆåŠŸæ¯”å¯¹ {aligned_count}/{len(gene_files)} ä¸ªåŸºå› ")
        if failed_genes:
            self.log(f"âŒ å¤±è´¥çš„åŸºå› : {', '.join(failed_genes)}")
        
        # ç¬¬äºŒæ­¥ï¼šå¹¶è¡ŒæŸ¥æ‰¾ä¿å®ˆåŒºåŸŸ
        self.log("ğŸ” å¼€å§‹æŸ¥æ‰¾ä¿å®ˆåŒºåŸŸ...")
        
        def find_conserved_regions_for_gene(alignment_file):
            """å¹¶è¡Œå¤„ç†å•ä¸ªåŸºå› çš„ä¿å®ˆåŒºåŸŸæŸ¥æ‰¾ - å¯»æ‰¾æœ€é•¿ä¿å®ˆåŒºåŸŸ"""
            gene_name = alignment_file.stem.replace("_aln", "")
            conserved_regions = []
            
            try:
                # æŸ¥æ‰¾ä¿å®ˆåŒºåŸŸ / Find conserved regions
                from Bio import AlignIO
                
                aln = AlignIO.read(alignment_file, "fasta")
                aln_len = aln.get_alignment_length()
                longest_conserved = None
                
                # ä»æœ€é•¿åˆ°æœ€çŸ­æœç´¢ä¿å®ˆåŒºåŸŸï¼Œç¡®ä¿æ‰¾åˆ°æœ€é•¿çš„ / Search from longest to shortest to ensure finding the longest
                # æœç´¢èŒƒå›´ï¼šä»åºåˆ—é•¿åº¦åˆ°50bp / Search range: from sequence length down to 50bp
                max_search_len = min(aln_len, 500)  # æœ€å¤§æœç´¢é•¿åº¦500bp / Maximum search length 500bp
                
                for win_len in range(max_search_len, 49, -1):  # ä»é•¿åˆ°çŸ­æœç´¢ / Search from long to short
                    found_at_this_length = False
                    
                    for start in range(aln_len - win_len + 1):
                        window_seqs = [str(rec.seq[start:start+win_len]).upper() for rec in aln]
                        
                        # æ£€æŸ¥æ˜¯å¦å®Œå…¨ä¸€è‡´ä¸”æ— gap / Check if completely identical and no gaps
                        if (len(set(window_seqs)) == 1 and  # æ‰€æœ‰åºåˆ—ä¸€è‡´ / All sequences identical
                            "-" not in window_seqs[0]):    # æ— gap / No gaps
                            
                            # æ‰¾åˆ°æœ€é•¿ä¿å®ˆåŒºåŸŸï¼Œç«‹å³è¿”å› / Found longest conserved region, return immediately
                            longest_conserved = {
                                'gene': gene_name,
                                'position': f"{start+1}-{start+win_len}",
                                'length': win_len,
                                'sequence': window_seqs[0]
                            }
                            found_at_this_length = True
                            break
                    
                    # å¦‚æœåœ¨å½“å‰é•¿åº¦æ‰¾åˆ°ä¿å®ˆåŒºåŸŸï¼Œè¿™å°±æ˜¯æœ€é•¿çš„ï¼Œåœæ­¢æœç´¢ / If found at current length, this is the longest, stop searching
                    if found_at_this_length:
                        break
                
                if longest_conserved:
                    conserved_regions.append(longest_conserved)
                    length = longest_conserved['length']
                    if length >= 200:
                        self.log(f"ğŸ¯ å‘ç°é•¿ä¿å®ˆåŒºåŸŸ: {gene_name} ({length}bp)")
                    elif length >= 100:
                        self.log(f"âœ… å‘ç°ä¿å®ˆåŒºåŸŸ: {gene_name} ({length}bp)")
                    elif length >= 80:
                        self.log(f"âœ… å‘ç°æ ‡å‡†ä¿å®ˆåŒºåŸŸ: {gene_name} ({length}bp)")
                    else:
                        self.log(f"âš ï¸  å‘ç°çŸ­ä¿å®ˆåŒºåŸŸ: {gene_name} ({length}bp)")
                else:
                    conserved_regions.append({
                        'gene': gene_name,
                        'position': 'æ— ä¿å®ˆåŒºåŸŸ',
                        'length': 0,
                        'sequence': ''
                    })
                    self.log(f"âŒ æ— ä¿å®ˆåŒºåŸŸ: {gene_name}")
                
                return gene_name, conserved_regions, longest_conserved is not None, None
                
            except Exception as e:
                self.log(f"âŒ åˆ†æä¿å®ˆåŒºåŸŸå¤±è´¥ {gene_name}: {e}", "ERROR")
                return gene_name, [{
                    'gene': gene_name,
                    'position': 'åˆ†æå¤±è´¥',
                    'length': 0,
                    'sequence': ''
                }], False, str(e)
        
        # å¹¶è¡ŒæŸ¥æ‰¾ä¿å®ˆåŒºåŸŸ
        all_conserved_regions = []
        conserved_count = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¿å®ˆåŒºåŸŸæŸ¥æ‰¾ä»»åŠ¡
            future_to_alignment = {
                executor.submit(find_conserved_regions_for_gene, alignment_file): alignment_file 
                for alignment_file in alignment_files
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_alignment):
                alignment_file = future_to_alignment[future]
                try:
                    gene_name, regions, success, error = future.result()
                    all_conserved_regions.extend(regions)
                    if success and regions and regions[0]['length'] > 0:
                        conserved_count += 1
                except Exception as exc:
                    gene_name = alignment_file.stem.replace("_aln", "")
                    self.log(f"âŒ {gene_name} ä¿å®ˆåŒºåŸŸåˆ†æå‡ºé”™: {exc}", "ERROR")
        
        # ä¿å­˜ä¿å®ˆåŒºåŸŸç»“æœ
        conserved_regions_file = self.genus_dir / "conserved_regions.txt"
        with open(conserved_regions_file, 'w') as conserved_f:
            conserved_f.write("Gene\tPosition\tLength\tSequence\n")
            for region in all_conserved_regions:
                conserved_f.write(f"{region['gene']}\t{region['position']}\t{region['length']}\t{region['sequence']}\n")
        
        self.log(f"âœ… æ­¥éª¤6å®Œæˆ: æ‰¾åˆ° {conserved_count} ä¸ªä¿å®ˆåŒºåŸŸ")
        self.log(f"ğŸ“ æ¯”å¯¹ç»“æœä¿å­˜åˆ°: {alignment_dir}")
        self.log(f"ğŸ“ ä¿å®ˆåŒºåŸŸç»“æœä¿å­˜åˆ°: {conserved_regions_file}")
        
        return conserved_count > 0
    
    def step7_primer3_design(self):
        """æ­¥éª¤7: ä½¿ç”¨Primer3è®¾è®¡å¼•ç‰© (å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬)"""
        self.log("ğŸš€ å¼€å§‹æ­¥éª¤7: Primer3å¼•ç‰©è®¾è®¡")
        
        conserved_regions_file = self.genus_dir / "conserved_regions.txt"
        if not conserved_regions_file.exists():
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°ä¿å®ˆåŒºåŸŸæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤6", "ERROR")
            return False
        
        # æ£€æŸ¥primer3æ˜¯å¦å¯ç”¨
        try:
            result = subprocess.run(["primer3_core", "--help"], capture_output=True, text=True)
            self.log("âœ… Primer3å¯ç”¨")
        except FileNotFoundError:
            self.log("âŒ æœªæ‰¾åˆ°primer3_coreç¨‹åºï¼Œè¯·å®‰è£…Primer3", "ERROR")
            return False
        
        # åˆ›å»ºprimer3ç›®å½•
        primer3_dir = self.genus_dir / "primer3_results"
        primer3_dir.mkdir(exist_ok=True)
        
        # è¯»å–ä¿å®ˆåŒºåŸŸ
        import pandas as pd
        conserved_df = pd.read_csv(conserved_regions_file, sep='\t')
        
        # è¿‡æ»¤æœ‰æ•ˆçš„ä¿å®ˆåŒºåŸŸ
        valid_regions = conserved_df[
            (conserved_df['Length'] >= 80) & 
            (conserved_df['Length'] <= 400) & 
            (conserved_df['Sequence'].notna())
        ]
        
        if len(valid_regions) == 0:
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä¿å®ˆåŒºåŸŸ", "ERROR")
            return False
        
        self.log(f"ğŸ“‹ æ‰¾åˆ° {len(valid_regions)} ä¸ªæœ‰æ•ˆä¿å®ˆåŒºåŸŸ")
        
        # å¹¶è¡Œè®¾è®¡å¼•ç‰©
        def design_primers_for_region(region_data):
            """å¹¶è¡Œå¤„ç†å•ä¸ªä¿å®ˆåŒºåŸŸçš„å¼•ç‰©è®¾è®¡"""
            idx, row = region_data
            gene_name = row['Gene']
            sequence = row['Sequence']
            
            # åˆ›å»ºä¸´æ—¶è¾“å…¥æ–‡ä»¶
            temp_input_file = primer3_dir / f"temp_input_{gene_name}_{idx}.txt"
            temp_output_file = primer3_dir / f"temp_output_{gene_name}_{idx}.txt"
            
            try:
                # å†™å…¥Primer3è¾“å…¥ (ä¼˜åŒ–äº§ç‰©å¤§å°é…ç½®)
                with open(temp_input_file, 'w') as f:
                    f.write(f"SEQUENCE_ID={gene_name}_{idx}\n")
                    f.write(f"SEQUENCE_TEMPLATE={sequence}\n")
                    f.write("PRIMER_TASK=generic\n")
                    f.write("PRIMER_PICK_LEFT_PRIMER=1\n")
                    f.write("PRIMER_PICK_INTERNAL_OLIGO=0\n")
                    f.write("PRIMER_PICK_RIGHT_PRIMER=1\n")
                    f.write("PRIMER_OPT_SIZE=20\n")
                    f.write("PRIMER_MIN_SIZE=18\n")
                    f.write("PRIMER_MAX_SIZE=25\n")
                    f.write("PRIMER_MAX_NS_ACCEPTED=0\n")
                    # ä¼˜åŒ–äº§ç‰©å¤§å°é…ç½® / Optimized product size configuration
                    f.write("PRIMER_PRODUCT_SIZE_RANGE=100-300 80-200 200-400\n")  # å¤šä¸ªèŒƒå›´é€‰æ‹© / Multiple range options
                    f.write("PRIMER_OPT_PRODUCT_SIZE=150\n")  # ç†æƒ³äº§ç‰©å¤§å° / Optimal product size
                    f.write("PRIMER_OPT_TM=60.0\n")
                    f.write("PRIMER_MIN_TM=55.0\n")
                    f.write("PRIMER_MAX_TM=65.0\n")
                    f.write("PRIMER_MIN_GC=30.0\n")
                    f.write("PRIMER_MAX_GC=70.0\n")
                    f.write("PRIMER_MAX_POLY_X=4\n")
                    # ç§»é™¤æœ‰é—®é¢˜çš„é…ç½®è·¯å¾„
                    # f.write("PRIMER_THERMODYNAMIC_PARAMETERS_PATH=/usr/share/primer3/primer3_config/\n")
                    f.write("=\n")
                
                # è¿è¡ŒPrimer3
                with open(temp_input_file, 'r') as input_f, open(temp_output_file, 'w') as output_f:
                    result = subprocess.run(
                        ["primer3_core"],
                        stdin=input_f,
                        stdout=output_f,
                        stderr=subprocess.PIPE,
                        text=True,
                        check=True
                    )
                
                # è¯»å–è¾“å‡ºç»“æœ
                with open(temp_output_file, 'r') as f:
                    output_content = f.read()
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                temp_input_file.unlink(missing_ok=True)
                temp_output_file.unlink(missing_ok=True)
                
                self.log(f"âœ… å®Œæˆå¼•ç‰©è®¾è®¡: {gene_name}_{idx}")
                return gene_name, idx, output_content, True, None
                
            except subprocess.CalledProcessError as e:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                temp_input_file.unlink(missing_ok=True)
                temp_output_file.unlink(missing_ok=True)
                
                # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                error_msg = f"Primer3 exit code: {e.returncode}"
                if e.stderr:
                    error_msg += f", stderr: {e.stderr[:200]}"
                
                self.log(f"âŒ å¼•ç‰©è®¾è®¡å¤±è´¥ {gene_name}_{idx}: {error_msg}", "ERROR")
                return gene_name, idx, "", False, error_msg
                
            except Exception as e:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                temp_input_file.unlink(missing_ok=True)
                temp_output_file.unlink(missing_ok=True)
                
                self.log(f"âŒ å¼•ç‰©è®¾è®¡å¤±è´¥ {gene_name}_{idx}: {e}", "ERROR")
                return gene_name, idx, "", False, str(e)
        
        # å‡†å¤‡å¹¶è¡Œä»»åŠ¡æ•°æ®
        region_tasks = [(idx, row) for idx, row in valid_regions.iterrows()]
        
        # è®¡ç®—åˆé€‚çš„å¹¶è¡Œæ•°é‡
        max_workers = min(self.threads, len(region_tasks))
        self.log(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹è¿›è¡Œå¼•ç‰©è®¾è®¡")
        
        # æ‰§è¡Œå¹¶è¡Œå¼•ç‰©è®¾è®¡
        all_outputs = []
        success_count = 0
        failed_regions = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰å¼•ç‰©è®¾è®¡ä»»åŠ¡
            future_to_region = {
                executor.submit(design_primers_for_region, region_data): region_data 
                for region_data in region_tasks
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_region):
                region_data = future_to_region[future]
                try:
                    gene_name, idx, output_content, success, error = future.result()
                    if success:
                        all_outputs.append(output_content)
                        success_count += 1
                    else:
                        failed_regions.append(f"{gene_name}_{idx}")
                        # è®°å½•ç¬¬ä¸€ä¸ªå¤±è´¥çš„è¯¦ç»†é”™è¯¯
                        if len(failed_regions) == 1:
                            self.log(f"ğŸ” ç¬¬ä¸€ä¸ªå¤±è´¥çš„è¯¦ç»†é”™è¯¯: {error}", "ERROR")
                except Exception as exc:
                    idx, row = region_data
                    gene_name = row['Gene']
                    self.log(f"âŒ {gene_name}_{idx} å¼•ç‰©è®¾è®¡å‡ºé”™: {exc}", "ERROR")
                    failed_regions.append(f"{gene_name}_{idx}")
        
        # åˆå¹¶æ‰€æœ‰è¾“å‡ºç»“æœ
        primer3_output_file = primer3_dir / "primer3_output.txt"
        with open(primer3_output_file, 'w') as f:
            for output in all_outputs:
                f.write(output)
                if not output.endswith('\n'):
                    f.write('\n')
        
        self.log(f"ğŸ“Š å¼•ç‰©è®¾è®¡å®Œæˆ: æˆåŠŸ {success_count}/{len(region_tasks)} ä¸ªåŒºåŸŸ")
        if failed_regions:
            self.log(f"âŒ å¤±è´¥çš„åŒºåŸŸ: {', '.join(failed_regions[:5])}{'...' if len(failed_regions) > 5 else ''}")
        
        if success_count > 0:
            self.log("âœ… Primer3è®¾è®¡å®Œæˆ")
            self.log(f"ğŸ“ ç»“æœä¿å­˜åˆ°: {primer3_output_file}")
            return True
        else:
            self.log("âŒ æ‰€æœ‰å¼•ç‰©è®¾è®¡éƒ½å¤±è´¥äº†", "ERROR")
            return False
    
    def step8_parse_primer3_results(self):
        """Step 8: Parse Primer3 results using toolkit (å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬)"""
        self.log("ğŸš€ Starting Step 8: Parse Primer3 results")
        
        primer3_dir = self.genus_dir / "primer3_results"
        primer3_output_file = primer3_dir / "primer3_output.txt"
        
        if not primer3_output_file.exists():
            self.log("âŒ Primer3 output file not found, please run step 7 first", "ERROR")
            return False
        
        # Use toolkit parser with parallel processing
        try:
            # è¯»å–Primer3è¾“å‡ºæ–‡ä»¶
            with open(primer3_output_file, 'r') as f:
                content = f.read()
            
            # åˆ†å‰²æˆç‹¬ç«‹çš„è®°å½•
            records = content.split('=\n')
            records = [record.strip() for record in records if record.strip()]
            
            self.log(f"ğŸ“‹ æ‰¾åˆ° {len(records)} ä¸ªPrimer3è®°å½•éœ€è¦è§£æ")
            
            if not records:
                self.log("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„Primer3è®°å½•", "ERROR")
                return False
            
            # å¹¶è¡Œè§£æPrimer3è®°å½•
            def parse_single_record(record):
                """å¹¶è¡Œå¤„ç†å•ä¸ªPrimer3è®°å½•çš„è§£æ"""
                try:
                    lines = record.split('\n')
                    result = {}
                    
                    for line in lines:
                        if '=' in line:
                            key, value = line.split('=', 1)
                            result[key] = value
                    
                    # æå–åŸºæœ¬ä¿¡æ¯
                    sequence_id = result.get('SEQUENCE_ID', '')
                    if not sequence_id:
                        return None, False, "ç¼ºå°‘SEQUENCE_ID"
                    
                    # è§£æå¼•ç‰©ä¿¡æ¯
                    primers = []
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¼•ç‰©è¿”å›
                    primer_returned = result.get('PRIMER_PAIR_NUM_RETURNED', '0')
                    if primer_returned == '0':
                        return {
                            'ID': sequence_id,
                            'Status': 'No primers found',
                            'Left_Primer': '',
                            'Right_Primer': '',
                            'Product_Size': 0,
                            'Left_Tm': 0,
                            'Right_Tm': 0,
                            'Left_GC': 0,
                            'Right_GC': 0
                        }, True, None
                    
                    # è§£æç¬¬ä¸€ä¸ªå¼•ç‰©å¯¹
                    primer_data = {
                        'ID': sequence_id,
                        'Status': 'Success',
                        'Left_Primer': result.get('PRIMER_LEFT_0_SEQUENCE', ''),
                        'Right_Primer': result.get('PRIMER_RIGHT_0_SEQUENCE', ''),
                        'Product_Size': int(result.get('PRIMER_PAIR_0_PRODUCT_SIZE', 0)),
                        'Left_Tm': float(result.get('PRIMER_LEFT_0_TM', 0)),
                        'Right_Tm': float(result.get('PRIMER_RIGHT_0_TM', 0)),
                        'Left_GC': float(result.get('PRIMER_LEFT_0_GC_PERCENT', 0)),
                        'Right_GC': float(result.get('PRIMER_RIGHT_0_GC_PERCENT', 0))
                    }
                    
                    return primer_data, True, None
                    
                except Exception as e:
                    return None, False, str(e)
            
            # å¹¶è¡Œè§£ææ‰€æœ‰è®°å½•
            max_workers = min(self.threads, len(records))
            self.log(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹è¿›è¡Œç»“æœè§£æ")
            
            parsed_results = []
            failed_count = 0
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰è§£æä»»åŠ¡
                future_to_record = {
                    executor.submit(parse_single_record, record): i 
                    for i, record in enumerate(records)
                }
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in as_completed(future_to_record):
                    record_idx = future_to_record[future]
                    try:
                        primer_data, success, error = future.result()
                        if success and primer_data:
                            parsed_results.append(primer_data)
                        else:
                            failed_count += 1
                            if error:
                                self.log(f"âŒ è®°å½• {record_idx} è§£æå¤±è´¥: {error}", "ERROR")
                    except Exception as exc:
                        failed_count += 1
                        self.log(f"âŒ è®°å½• {record_idx} è§£æå‡ºé”™: {exc}", "ERROR")
            
            # ä¿å­˜è§£æç»“æœ
            parsed_results_file = primer3_dir / "parsed_primers.csv"
            
            if parsed_results:
                import pandas as pd
                df = pd.DataFrame(parsed_results)
                df.to_csv(parsed_results_file, index=False)
                
                self.log(f"ğŸ“Š è§£æå®Œæˆ: æˆåŠŸ {len(parsed_results)}/{len(records)} ä¸ªè®°å½•")
                if failed_count > 0:
                    self.log(f"âŒ å¤±è´¥è®°å½•æ•°: {failed_count}")
                
                self.log("âœ… Primer3 results parsing completed")
                self.log(f"ğŸ“ Parsed results saved to: {parsed_results_file}")
                return True
            else:
                self.log("âŒ æ²¡æœ‰æˆåŠŸè§£æä»»ä½•è®°å½•", "ERROR")
                return False
            
        except Exception as e:
            self.log(f"âŒ Failed to parse Primer3 results: {e}", "ERROR")
            return False
    
    def step9_rank_primers(self):
        """Step 9: Primer quality scoring and ranking (å¹¶è¡Œä¼˜åŒ–ç‰ˆæœ¬)"""
        self.log("ğŸš€ Starting Step 9: Primer quality scoring and ranking")
        
        primer3_dir = self.genus_dir / "primer3_results"
        parsed_results_file = primer3_dir / "parsed_primers.csv"
        
        if not parsed_results_file.exists():
            self.log("âŒ Parsed primer file not found, please run step 8 first", "ERROR")
            return False
        
        # è¯»å–è§£æç»“æœ
        try:
            import pandas as pd
            df = pd.read_csv(parsed_results_file)
            
            if len(df) == 0:
                self.log("âŒ æ²¡æœ‰æ‰¾åˆ°å¼•ç‰©æ•°æ®", "ERROR")
                return False
            
            self.log(f"ğŸ“‹ éœ€è¦è¯„åˆ† {len(df)} ä¸ªå¼•ç‰©")
            
            # å¹¶è¡Œè®¡ç®—è´¨é‡è¯„åˆ†
            def calculate_quality_score(primer_data):
                """å¹¶è¡Œè®¡ç®—å•ä¸ªå¼•ç‰©çš„è´¨é‡è¯„åˆ†"""
                try:
                    idx, row = primer_data
                    
                    # æ£€æŸ¥å¼•ç‰©æ˜¯å¦æˆåŠŸè®¾è®¡
                    if row['Status'] != 'Success' or not row['Left_Primer'] or not row['Right_Primer']:
                        return idx, {
                            'Quality_Score': 0,
                            'Tm_Score': 0,
                            'GC_Score': 0,
                            'Length_Score': 0,
                            'Product_Score': 0,
                            'Overall_Grade': 'Failed'
                        }, True, None
                    
                    # è®¡ç®—å„é¡¹è¯„åˆ†
                    scores = {}
                    
                    # 1. Tmè¯„åˆ† (ç†æƒ³èŒƒå›´: 55-65Â°C)
                    left_tm = float(row['Left_Tm'])
                    right_tm = float(row['Right_Tm'])
                    tm_diff = abs(left_tm - right_tm)
                    
                    # Tmä¸€è‡´æ€§è¯„åˆ† (å·®å¼‚è¶Šå°è¶Šå¥½)
                    if tm_diff <= 2:
                        tm_consistency = 100
                    elif tm_diff <= 5:
                        tm_consistency = 80
                    elif tm_diff <= 10:
                        tm_consistency = 60
                    else:
                        tm_consistency = 40
                    
                    # TmèŒƒå›´è¯„åˆ†
                    avg_tm = (left_tm + right_tm) / 2
                    if 58 <= avg_tm <= 62:
                        tm_range = 100
                    elif 55 <= avg_tm <= 65:
                        tm_range = 80
                    elif 50 <= avg_tm <= 70:
                        tm_range = 60
                    else:
                        tm_range = 40
                    
                    scores['Tm_Score'] = (tm_consistency + tm_range) / 2
                    
                    # 2. GCå«é‡è¯„åˆ† (ç†æƒ³èŒƒå›´: 40-60%)
                    left_gc = float(row['Left_GC'])
                    right_gc = float(row['Right_GC'])
                    avg_gc = (left_gc + right_gc) / 2
                    
                    if 45 <= avg_gc <= 55:
                        scores['GC_Score'] = 100
                    elif 40 <= avg_gc <= 60:
                        scores['GC_Score'] = 80
                    elif 35 <= avg_gc <= 65:
                        scores['GC_Score'] = 60
                    else:
                        scores['GC_Score'] = 40
                    
                    # 3. å¼•ç‰©é•¿åº¦è¯„åˆ† (ç†æƒ³é•¿åº¦: 18-25bp)
                    left_len = len(row['Left_Primer'])
                    right_len = len(row['Right_Primer'])
                    avg_len = (left_len + right_len) / 2
                    
                    if 19 <= avg_len <= 22:
                        scores['Length_Score'] = 100
                    elif 18 <= avg_len <= 25:
                        scores['Length_Score'] = 80
                    elif 16 <= avg_len <= 28:
                        scores['Length_Score'] = 60
                    else:
                        scores['Length_Score'] = 40
                    
                    # 4. äº§ç‰©å¤§å°è¯„åˆ† (ç†æƒ³èŒƒå›´: 100-200bp)
                    product_size = int(row['Product_Size'])
                    if 120 <= product_size <= 180:
                        scores['Product_Score'] = 100
                    elif 100 <= product_size <= 200:
                        scores['Product_Score'] = 80
                    elif 80 <= product_size <= 250:
                        scores['Product_Score'] = 60
                    else:
                        scores['Product_Score'] = 40
                    
                    # 5. è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ† (åŠ æƒå¹³å‡)
                    weights = {
                        'Tm_Score': 0.3,
                        'GC_Score': 0.25,
                        'Length_Score': 0.2,
                        'Product_Score': 0.25
                    }
                    
                    quality_score = sum(scores[key] * weights[key] for key in weights)
                    scores['Quality_Score'] = round(quality_score, 2)
                    
                    # 6. æ€»ä½“ç­‰çº§è¯„å®š
                    if quality_score >= 85:
                        scores['Overall_Grade'] = 'Excellent'
                    elif quality_score >= 75:
                        scores['Overall_Grade'] = 'Good'
                    elif quality_score >= 65:
                        scores['Overall_Grade'] = 'Fair'
                    elif quality_score >= 50:
                        scores['Overall_Grade'] = 'Poor'
                    else:
                        scores['Overall_Grade'] = 'Very Poor'
                    
                    return idx, scores, True, None
                    
                except Exception as e:
                    return idx, {}, False, str(e)
            
            # å¹¶è¡Œè®¡ç®—æ‰€æœ‰å¼•ç‰©çš„è´¨é‡è¯„åˆ†
            max_workers = min(self.threads, len(df))
            self.log(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹è¿›è¡Œè´¨é‡è¯„åˆ†")
            
            all_scores = {}
            failed_count = 0
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰è¯„åˆ†ä»»åŠ¡
                future_to_primer = {
                    executor.submit(calculate_quality_score, (idx, row)): idx 
                    for idx, row in df.iterrows()
                }
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in as_completed(future_to_primer):
                    primer_idx = future_to_primer[future]
                    try:
                        idx, scores, success, error = future.result()
                        if success:
                            all_scores[idx] = scores
                        else:
                            failed_count += 1
                            if error:
                                self.log(f"âŒ å¼•ç‰© {idx} è¯„åˆ†å¤±è´¥: {error}", "ERROR")
                    except Exception as exc:
                        failed_count += 1
                        self.log(f"âŒ å¼•ç‰© {primer_idx} è¯„åˆ†å‡ºé”™: {exc}", "ERROR")
            
            # å°†è¯„åˆ†ç»“æœæ·»åŠ åˆ°DataFrame
            for score_key in ['Quality_Score', 'Tm_Score', 'GC_Score', 'Length_Score', 'Product_Score', 'Overall_Grade']:
                df[score_key] = df.index.map(lambda idx: all_scores.get(idx, {}).get(score_key, 0))
            
            # æŒ‰è´¨é‡è¯„åˆ†æ’åº
            df_sorted = df.sort_values('Quality_Score', ascending=False)
            
            # ä¿å­˜æ’åºç»“æœ
            ranked_results_file = primer3_dir / "ranked_primers.csv"
            df_sorted.to_csv(ranked_results_file, index=False)
            
            self.log(f"ğŸ“Š è´¨é‡è¯„åˆ†å®Œæˆ: æˆåŠŸ {len(df) - failed_count}/{len(df)} ä¸ªå¼•ç‰©")
            if failed_count > 0:
                self.log(f"âŒ å¤±è´¥å¼•ç‰©æ•°: {failed_count}")
            
            self.log("âœ… Primer quality scoring and ranking completed")
            self.log(f"ğŸ“ Final results saved to: {ranked_results_file}")
            
            # Display top primers summary
            try:
                if len(df_sorted) > 0:
                    self.log(f"ğŸ† Total designed primers: {len(df_sorted)}")
                    self.log("ğŸ¥‡ Top 3 best primers:")
                    for i in range(min(3, len(df_sorted))):
                        row = df_sorted.iloc[i]
                        self.log(f"   {i+1}. {row['ID']} - Quality Score: {row.get('Quality_Score', 'N/A'):.2f} ({row.get('Overall_Grade', 'N/A')})")
                        
                    # Quality summary
                    grade_counts = df_sorted['Overall_Grade'].value_counts()
                    self.log(f"ğŸ“Š Quality Summary:")
                    for grade, count in grade_counts.items():
                        self.log(f"   {grade}: {count}")
                        
            except Exception as e:
                self.log(f"âš ï¸  Error displaying results summary: {e}")
            
            return True
            
        except Exception as e:
            self.log(f"âŒ Failed to rank primers: {e}", "ERROR")
            return False
    
    def expand_gene_selection(self):
        """åœ¨å¼•ç‰©è®¾è®¡å¤±è´¥æ—¶æ‰©å±•åŸºå› é€‰æ‹©èŒƒå›´"""
        self.log("ğŸ”„ å¼•ç‰©è®¾è®¡æ•ˆæœä¸ä½³ï¼Œå°è¯•æ‰©å±•åŸºå› é€‰æ‹©èŒƒå›´")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„æ‰€æœ‰ç‰¹å¼‚æ€§åŸºå› ä¿¡æ¯
        detailed_genes_file = self.genus_dir / "specific_genes_detailed.csv"
        if not detailed_genes_file.exists():
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°è¯¦ç»†åŸºå› ä¿¡æ¯æ–‡ä»¶", "ERROR")
            return False
        
        # æ£€æŸ¥roaryç»“æœ
        gene_presence_file = None
        for roary_dir in [self.roary_dir] + list(self.work_dir.glob("roary_*")):
            potential_file = roary_dir / "gene_presence_absence.csv"
            if potential_file.exists():
                gene_presence_file = potential_file
                break
        
        if not gene_presence_file:
            self.log("âŒ æ²¡æœ‰æ‰¾åˆ°gene_presence_absence.csvæ–‡ä»¶", "ERROR")
            return False
        
        import pandas as pd
        df = pd.read_csv(gene_presence_file, low_memory=False)
        
        # è·å–æ ·æœ¬åˆ—å
        sample_columns = [col for col in df.columns if col not in 
                         ['Gene', 'Non-unique Gene name', 'Annotation', 'No. isolates', 'No. sequences', 
                          'Avg sequences per isolate', 'Genome Fragment', 'Order within Fragment', 
                          'Accessory Fragment', 'Accessory Order with Fragment', 'QC', 'Min group size nuc', 
                          'Max group size nuc', 'Avg group size nuc']]
        
        # é‡æ–°åˆ†ç±»æ ·æœ¬
        target_prefix = self.genus[:3].capitalize() + "_"
        target_samples = [sample for sample in sample_columns if sample.startswith(target_prefix)]
        outgroup_samples = [sample for sample in sample_columns if not sample.startswith(target_prefix)]
        
        # é‡æ–°ç­›é€‰æ‰€æœ‰ç‰¹å¼‚æ€§åŸºå› 
        all_specific_genes = []
        for idx, row in df.iterrows():
            gene_name = row['Gene']
            
            target_present = all(pd.notna(row[sample]) and str(row[sample]).strip() != '' 
                               for sample in target_samples)
            
            if outgroup_samples:
                outgroup_absent = all(pd.isna(row[sample]) or str(row[sample]).strip() == '' 
                                    for sample in outgroup_samples)
            else:
                outgroup_absent = True
            
            if target_present and outgroup_absent:
                all_specific_genes.append(gene_name)
        
        # æ›´æ–°åŸºå› åˆ—è¡¨
        specific_genes_file = self.genus_dir / "specific_genes.txt"
        with open(specific_genes_file, 'w') as f:
            for gene in all_specific_genes:
                f.write(f"{gene}\n")
        
        # æ›´æ–°è¯¦ç»†ä¿¡æ¯
        selected_df = df[df['Gene'].isin(all_specific_genes)][['Gene', 'Annotation']].copy()
        selected_df.to_csv(detailed_genes_file, index=False)
        
        self.log(f"âœ… å·²æ‰©å±•åˆ°æ‰€æœ‰ {len(all_specific_genes)} ä¸ªç‰¹å¼‚æ€§åŸºå› ")
        self.log(f"ğŸ“ æ›´æ–°çš„åŸºå› åˆ—è¡¨ä¿å­˜åˆ°: {specific_genes_file}")
        
        return True
    
    def run_pipeline(self, start_step=1):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        self.log(f"ğŸ¯ å¼€å§‹è¿è¡ŒåŸºå› ç»„åˆ†ææµç¨‹ - {self.genus}")
        self.log(f"ğŸ“ å·¥ä½œç›®å½•: {self.work_dir}")
        self.log(f"ğŸ”¢ ä»æ­¥éª¤ {start_step} å¼€å§‹")
        
        # åˆ›å»ºå·¥ä½œç›®å½•
        self.genus_dir.mkdir(exist_ok=True)
        
        steps = {
            1: ("ä¸‹è½½åŸºå› ç»„", self.step1_download),
            2: ("Prokkaæ³¨é‡Š", self.step2_prokka),
            3: ("Roaryæ³›åŸºå› ç»„åˆ†æ", self.step3_roary),
            4: ("ç­›é€‰ç›®æ ‡ç‰¹å¼‚æ€§åŸºå› ", self.step4_find_specific_genes),
            5: ("æå–åŸºå› åºåˆ—å¹¶è¿›è¡Œå¤šåºåˆ—æ¯”å¯¹", self.step5_extract_gene_sequences),
            6: ("å¤šåºåˆ—æ¯”å¯¹å¹¶æ‰¾åˆ°ä¿å®ˆåŒºåŸŸ", self.step6_find_conserved_regions),
            7: ("ä½¿ç”¨Primer3è®¾è®¡å¼•ç‰©", self.step7_primer3_design),
            8: ("è§£æPrimer3ç»“æœ", self.step8_parse_primer3_results),
            9: ("å¼•ç‰©è´¨é‡è¯„åˆ†å’Œæ’åº", self.step9_rank_primers)
        }
        
        for step_num in range(start_step, 10):
            step_name, step_func = steps[step_num]
            self.log(f"\n{'='*50}")
            self.log(f"æ‰§è¡Œæ­¥éª¤ {step_num}: {step_name}")
            self.log(f"{'='*50}")
            
            if not step_func():
                self.log(f"âŒ æ­¥éª¤ {step_num} å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢", "ERROR")
                return False
        
        self.log("\nğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
        self.log("ğŸ“ ç»“æœæ–‡ä»¶ä½ç½®:")
        self.log(f"   - åŸºå› ç»„æ–‡ä»¶: {self.data_dir}")
        
        # æ˜¾ç¤ºåŸºå› ç»„summaryæ–‡ä»¶
        summary_files = []
        target_summary = self.genus_dir / f"{self.genus_safe}_genome_summary.csv"
        if target_summary.exists():
            summary_files.append(f"   - ç›®æ ‡åŸºå› ç»„summary: {target_summary}")
        
        for outgroup, outgroup_safe in zip(self.outgroup_genera, self.outgroup_genera_safe):
            outgroup_dir = self.work_dir / outgroup_safe
            outgroup_summary = outgroup_dir / f"{outgroup_safe}_genome_summary.csv"
            if outgroup_summary.exists():
                summary_files.append(f"   - å¤–ç¾¤åŸºå› ç»„summary: {outgroup_summary}")
        
        if summary_files:
            for summary_file in summary_files:
                self.log(summary_file)
        
        self.log(f"   - Prokkaæ³¨é‡Š: {self.prokka_dir}")
        
        # æ£€æŸ¥roaryç»“æœç›®å½•
        roary_result_dir = None
        for roary_dir in [self.roary_dir] + list(self.work_dir.glob("roary_*")):
            if (roary_dir / "gene_presence_absence.csv").exists():
                roary_result_dir = roary_dir
                break
        if roary_result_dir:
            self.log(f"   - Roaryç»“æœ: {roary_result_dir}")
        
        # æ£€æŸ¥å¼•ç‰©è®¾è®¡ç»“æœ
        if (self.genus_dir / "specific_genes.txt").exists():
            self.log(f"   - ç‰¹å¼‚æ€§åŸºå› : {self.genus_dir / 'specific_genes.txt'}")
        if (self.genus_dir / "gene_sequences").exists():
            self.log(f"   - åŸºå› åºåˆ—: {self.genus_dir / 'gene_sequences'}")
        if (self.genus_dir / "conserved_regions.txt").exists():
            self.log(f"   - ä¿å®ˆåŒºåŸŸ: {self.genus_dir / 'conserved_regions.txt'}")
        if (self.genus_dir / "primer3_results" / "ranked_primers.csv").exists():
            self.log(f"   - æœ€ç»ˆå¼•ç‰©: {self.genus_dir / 'primer3_results' / 'ranked_primers.csv'}")
        
        return True

def main():
    parser = argparse.ArgumentParser(
        description="å®Œæ•´çš„åŸºå› ç»„åˆ†ææµç¨‹ï¼šä¸‹è½½ -> Prokka -> Roary -> å¼•ç‰©è®¾è®¡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å®Œæ•´æµç¨‹ (ä¸‹è½½ -> Prokka -> Roary -> å¼•ç‰©è®¾è®¡ï¼Œé»˜è®¤å¿«é€Ÿæ¨¡å¼)
  python complete_pipeline.py Intestinibacter --outgroup Clostridium Bacteroides
  
  # ä½¿ç”¨é«˜è´¨é‡æ¨¡å¼ (é€‚åˆæœ€ç»ˆåˆ†æå’Œå¼•ç‰©è®¾è®¡)
  python complete_pipeline.py Intestinibacter --high-quality
  
  # ä»æ­¥éª¤2å¼€å§‹ï¼ˆè·³è¿‡ä¸‹è½½ï¼‰
  python complete_pipeline.py Intestinibacter --start-step 2
  
  # åªè¿è¡ŒRoaryåˆ†æ (å¿«é€Ÿæ¨¡å¼)
  python complete_pipeline.py Intestinibacter --start-step 3 --threads 50
  
  # ä»å¼•ç‰©è®¾è®¡å¼€å§‹ (å‡è®¾å·²æœ‰Roaryç»“æœ)
  python complete_pipeline.py Intestinibacter --start-step 4
  
  # åªè¿è¡Œå¼•ç‰©è´¨é‡è¯„åˆ† (å‡è®¾å·²æœ‰è§£æç»“æœ)
  python complete_pipeline.py Intestinibacter --start-step 9

åŸºå› é€‰æ‹©ç­–ç•¥:
  æ­¥éª¤4ä¼šæ™ºèƒ½é€‰æ‹©ç‰¹å¼‚æ€§åŸºå› ç”¨äºå¼•ç‰©è®¾è®¡ï¼š
  1. ä¼˜å…ˆé€‰æ‹©hypothetical proteinåŸºå› (â‰¥5ä¸ª)
  2. å¦‚æœhypothetical proteinä¸è¶³ï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰ç‰¹å¼‚æ€§åŸºå› 
  3. å¦‚æœå¼•ç‰©è®¾è®¡æ•ˆæœä¸ä½³ï¼Œå¯ä»¥æ‰‹åŠ¨æ‰©å±•åŸºå› èŒƒå›´ï¼š
     - ä¿®æ”¹specific_genes.txtæ–‡ä»¶
     - æˆ–è€…é‡æ–°è¿è¡Œæ­¥éª¤5-9

å¤–ç¾¤è¯†åˆ«:
  è‡ªåŠ¨è¯†åˆ«ï¼šç›®æ ‡genuså‰ç¼€å¼€å¤´çš„ä¸ºç›®æ ‡åŸºå› ç»„ï¼Œå…¶ä»–ä¸ºå¤–ç¾¤
  ä¾‹å¦‚ï¼šTerrisporobacter -> Ter_å¼€å¤´ä¸ºç›®æ ‡ï¼Œå…¶ä»–(å¦‚Int_)ä¸ºå¤–ç¾¤
  
æ­¥éª¤è¯´æ˜:
  1: ä¸‹è½½åŸºå› ç»„
  2: Prokkaæ³¨é‡Š
  3: Roaryæ³›åŸºå› ç»„åˆ†æ
  4: ç­›é€‰ç›®æ ‡ç‰¹å¼‚æ€§åŸºå›  (ä¼˜å…ˆhypothetical protein)
  5: æå–åŸºå› åºåˆ—
  6: å¤šåºåˆ—æ¯”å¯¹å’Œä¿å®ˆåŒºåŸŸè¯†åˆ«
  7: Primer3å¼•ç‰©è®¾è®¡
  8: è§£æPrimer3ç»“æœ
  9: å¼•ç‰©è´¨é‡è¯„åˆ†å’Œæ’åº
        """
    )
    
    parser.add_argument("genus", help="ç›®æ ‡genusåç§°")
    parser.add_argument("--outgroup", nargs="*", help="å¤–ç¾¤genusåç§°åˆ—è¡¨")
    parser.add_argument("--level", default="Complete Genome", help="ç»„è£…çº§åˆ« (é»˜è®¤: 'Complete Genome')")
    parser.add_argument("--threads", type=int, default=4, help="å¹¶è¡Œçº¿ç¨‹æ•° (é»˜è®¤: 4)")
    parser.add_argument("--start-step", type=int, choices=[1, 2, 3, 4, 5, 6, 7, 8, 9], default=1,
                       help="å¼€å§‹æ­¥éª¤: 1=ä¸‹è½½, 2=Prokka, 3=Roary, 4=ç­›é€‰ç›®æ ‡ç‰¹å¼‚æ€§åŸºå› , 5=æå–åŸºå› åºåˆ—å¹¶è¿›è¡Œå¤šåºåˆ—æ¯”å¯¹, 6=å¤šåºåˆ—æ¯”å¯¹å¹¶æ‰¾åˆ°ä¿å®ˆåŒºåŸŸ, 7=ä½¿ç”¨Primer3è®¾è®¡å¼•ç‰©, 8=è§£æPrimer3ç»“æœ, 9=å¼•ç‰©è´¨é‡è¯„åˆ†å’Œæ’åº (é»˜è®¤: 1)")
    parser.add_argument("--fast", action="store_true", default=True,
                       help="ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ (-n) è¿›è¡ŒRoaryåˆ†æ (é»˜è®¤: True)")
    parser.add_argument("--high-quality", action="store_true", 
                       help="ä½¿ç”¨é«˜è´¨é‡æ¨¡å¼ (--mafft) è¿›è¡ŒRoaryåˆ†æï¼Œè¦†ç›– --fast")
    
    args = parser.parse_args()
    
    # ç¡®å®šä½¿ç”¨å“ªç§æ¨¡å¼
    fast_mode = not args.high_quality  # å¦‚æœæŒ‡å®šäº†high_qualityï¼Œå°±ä¸ä½¿ç”¨fastæ¨¡å¼
    
    # åˆ›å»ºæµç¨‹å®ä¾‹
    pipeline = GenomePipeline(
        genus=args.genus,
        outgroup_genera=args.outgroup,
        level=args.level,
        threads=args.threads,
        fast_mode=fast_mode
    )
    
    # è¿è¡Œæµç¨‹
    success = pipeline.run_pipeline(start_step=args.start_step)
    
    if not success:
        sys.exit(1)

if __name__ == "__main__":
    main() 